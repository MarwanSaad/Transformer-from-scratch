{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Building a transformer from scratch for multi-label classification task**\n",
    "#### **Sections:**\n",
    " - Importing libraries\n",
    " - Data Cleaning & Analysis\n",
    " - Transformer Blocks\n",
    " - Preparing Train/Test Data\n",
    " - Model Training\n",
    " - Model Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Importing Libraries** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  #For numerical operations\n",
    "import pandas as pd    #For data manipulation\n",
    "import string          #For string operations\n",
    "import re              #python regular expressions\n",
    "import nltk            #For NLP operations\n",
    "from nltk.corpus import stopwords    #For removing stopwords from text\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import WordNetLemmatizer   #For text lemmatization\n",
    "nltk.download('wordnet')\n",
    "from collections import Counter    #For counting words in a list\n",
    "\n",
    "import torch  #Pytorch for neural networks\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split  #For handling dataset and batches for training\n",
    "from sklearn.model_selection import train_test_split    #For train/test data splitting\n",
    "import math    #For mathematical operations\n",
    "\n",
    "from transformers import BertTokenizer  #For text tokenization\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score  #Metrics for evaluating the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data Cleaning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating functions for cleaning text from punctuations, special characters, text tokenization, removing stop words and text lemmatization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''\n",
    "    This function cleans a text by by making the following operations:\n",
    "    1- Transform to lower case\n",
    "    2- Remove punctuation\n",
    "    3- Remove special characters\n",
    "    4- removing repetitive letter sequences [ex: extrrrra --> extra]\n",
    "    '''\n",
    "    text = text.lower()   #tranforming all text to lower case\n",
    "    \n",
    "    #using python list comprehension to fill a list with all words\n",
    "    #without punctuations and special characters included in the string library\n",
    "    cleaned_text_list = [words for words in text if words not in string.punctuation] \n",
    "    \n",
    "    #reforming the string by joining all the words in the list\n",
    "    cleaned_text = ''.join(cleaned_text_list) \n",
    "    \n",
    "    #using regular expression to substitute '\\n' and '\\t' with spaces\n",
    "    cleaned_text = re.sub(r\"\\n\",\" \",cleaned_text)\n",
    "    cleaned_text = re.sub(r\"\\t\",\" \",cleaned_text)\n",
    "    cleaned_text = re.sub(r'(.)\\1+', r'\\1', cleaned_text)  #removing repetitive letters\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    '''\n",
    "    This function is responsible for the tokenization process.\n",
    "    which is splitting the strings into a list of words\n",
    "    '''\n",
    "    #using the split function to separate words\n",
    "    #'\\W+' splits the words on non-words characters\n",
    "    splitted_text = re.split('\\W+',text) \n",
    "    return splitted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(text):\n",
    "    '''\n",
    "    This function removes stop words from the text using the NLTK library\n",
    "    some example stop words are: i, me, you, my, myself\n",
    "    '''\n",
    "    #all english language stop words in the library\n",
    "    stopword = nltk.corpus.stopwords.words('english')  \n",
    "    \n",
    "    #creating a list with all words, excluding the stop words\n",
    "    cleaned_text = [word for word in text if word not in stopword]\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(text):\n",
    "    '''\n",
    "    This function applies lemmatization operation on the text\n",
    "    which is reducing the word to its root form\n",
    "    '''\n",
    "    ##ss = SnowballStemmer(language='english') #creating instance\n",
    "    #creating a list with applying stemming to each word\n",
    "    #stemmed_words = [ss.stem(word) for word in text ]\n",
    "    #joined_text = ' '.join(stemmed_words) \n",
    "    lemm = WordNetLemmatizer()\n",
    "    lemmatized_text = ' '.join([lemm.lemmatize(word) for word in text])\n",
    "    #joined_text = ' '.join(lemmatized_text)\n",
    "    return lemmatized_text\n",
    "\n",
    "#Lemmatization resulted in much better results on text than stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_data = pd.read_csv('train.csv')\n",
    "og_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There are two string columns and six integer columns, and no Missing values in the data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected results from applying the following functions to the text:**  \n",
    "\n",
    "clean_text(): Return the text lowered cased, removed punctuations, special characters and repetitive letters  \n",
    "\n",
    "tokenize_text(): Splits the text into individual words and return a list of words  \n",
    "\n",
    "remove_stop_words(): return a list of words with all stopwords removed  \n",
    "\n",
    "lemmatize(): lemmatize the words in the list and join them back into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying the clean_text() function on the data\n",
    "og_data['comment_text'] = og_data['comment_text'].apply(lambda text:clean_text(text))\n",
    "og_data['comment_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_data['comment_text'] = og_data['comment_text'].apply(lambda text:tokenize(text))\n",
    "og_data['comment_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_data['comment_text'] = og_data['comment_text'].apply(lambda text:remove_stop_words(text))\n",
    "og_data['comment_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_data['tokenized_txt'] = og_data['comment_text']  #Creating a new column with the tokenized text to be used in analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_data['comment_text'] = og_data['comment_text'].apply(lambda text:lemmatize(text))\n",
    "og_data['comment_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The text is now fully cleaned and lemmatized**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I retured the text as sentences and not tokenized as I will use the bert tokenizer. I found that it was used in most resources, so just to make sure the the text is tokenized correctly for the training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_columns = og_data.columns.tolist()[2:8]  #creating a list of the label names\n",
    "label_counts = og_data[labels_columns].sum().sort_values()  #counting the number of occurences of a label and sorting them\n",
    "labels_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dividing the data to toxic and clean text to check for data imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_data = og_data[og_data[labels_columns].sum(axis=1)>0] #filtering for toxic data only, where it will have a value of 1 in label columns\n",
    "clean_data = og_data[og_data[labels_columns].sum(axis=1)==0] #filtering for clean data only, where it will have a value of 0 in label columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(toxic_data.shape)\n",
    "print(clean_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a big difference between toxic and clean text  \n",
    "\n",
    "Handling data imbalance:  \n",
    "Implementing Random Under Sampling, which is randomly sampling from the majority class which is the clean comments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sampling the clean data\n",
    "sampled_clean = clean_data.sample(n=16225, random_state=42)\n",
    "#combine toxic and the sampled clean data\n",
    "balanced_df = pd.concat([toxic_data, sampled_clean], axis=0)\n",
    "#Shuffling the data\n",
    "balanced_df = balanced_df.sample(frac=1, random_state=42)\n",
    "\n",
    "balanced_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(toxic_data.shape)\n",
    "print(sampled_clean.shape)\n",
    "print(balanced_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is now a balanced dataframe of total 32450 samples of text  \n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Finding most common words in each class to find if a word can belong to multiple classes  \n",
    " this is by looping over the text data of each label, inserting the words in a list and counting the most common 5 words in each label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in labels_columns:\n",
    "    words = []\n",
    "    for comment in balanced_df[balanced_df[label]==1]['tokenized_txt']:\n",
    "        words.extend(comment)\n",
    "    common_words = Counter(words).most_common(5)\n",
    "    print(f'Common words in the {label} class: {common_words}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A word can belong to multible labels at the same time  \n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Transformer Blocks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section includes the main building blocks of a transformer which are:  \n",
    "- Positional Encoding\n",
    "- Feed Forward\n",
    "- Multi Head Attention\n",
    "- Encoder\n",
    "- Decoder\n",
    "- Combined Transformer  \n",
    "\n",
    "The following code cells contains classes for each building block  \n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Positional Encoding block structure:**  \n",
    "- The constructor takes inputs: d_model which is the model dimension, seq_len which is the maximum sequence length and dropout for regularization\n",
    "- create a tensor of zeros to be filled with the positional encodings\n",
    "- Calculate the positional encoding based on the formulas from the paper\n",
    "- Adds the positional encoding to the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Positional Encoding class\n",
    "\n",
    "#expected to create a vector of same size d_model, that tells the model the position of a particular word in a sentence\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    #seq_len: max length of the sentence, dropout make the model less overfit\n",
    "    def __init__(self, d_model: int, seq_len: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Tensor of zeros, will be filled with positional encodings\n",
    "        pe = torch.zeros(seq_len, d_model)\n",
    "\n",
    "        #The formulas used in the paper to calculate the positional encodings\n",
    "\n",
    "        # A vector of shape (seq_len - 1)\n",
    "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n",
    "        # The denomenator of the formula\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "\n",
    "        #Applying sin to even positions and cos to odd positions\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        #Keeping the tensor to be saved with the model but not as a learnable parameter\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #Adding positional encoding to the input\n",
    "        x = x+ (self.pe[:, :x.shape[1]]).requires_grad_(False) # making this tensor fixed not a learnable parameter\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feed Forward Block structure:** \n",
    "- The constructor takes two inputs: d_model and d_ff which is the dimension of the inner layer\n",
    "- Creates 2 fully connected layers\n",
    "- Defines the activation function\n",
    "- the input passes to the 1st layer --> activation function --> 2nd layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feed Forward Block\n",
    "class FeedForwardBlock(nn.Module):\n",
    "    #d_ff: dimensionality of the inner layer\n",
    "    def __init__(self, d_model: int, d_ff: int) -> None:\n",
    "        super().__init__()\n",
    "        #two fully connnected layers\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff,d_model)\n",
    "        #activation function\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self,x):\n",
    "        #input x -> fc1 -> activation function -> fc2\n",
    "        return self.fc2(self.relu(self.fc1(x)))\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multi-head Attention Block Structure:**  \n",
    "- constructor takes two inputs: d_model and num_heads(number of attention heads)\n",
    "- apply linear transformation to the input Q,k,v\n",
    "- split to num_heads\n",
    "- apply the scaled dot product\n",
    "- apply linear transformation and combine back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multi head attention\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        # Check if d_model is divisble by number of heads(num_heads)\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model # model dimension\n",
    "        self.num_heads = num_heads # num of attention heads\n",
    "        self.d_k = d_model // num_heads # dimension of k,q,v\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model) # query linear transformation\n",
    "        self.W_k = nn.Linear(d_model, d_model) # key linear transformation\n",
    "        self.W_v = nn.Linear(d_model, d_model) # value linear transformation\n",
    "        self.W_o = nn.Linear(d_model, d_model) # output linear transformation\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        \n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)  #calculating the attention scores with matrix multiplication\n",
    "        \n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)   #Applying mask to filter out unnecessary part like padding\n",
    "            #the mask sets the padding to a very large negative value which then is turned to zero by the softmax function\n",
    "        \n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)   #applying softmax to calculate the probability\n",
    "        \n",
    "        output = torch.matmul(attn_probs, V)  #multiply the probabilities by the value \n",
    "        return output\n",
    "        \n",
    "    def split_heads(self, x):  #splits the input into multiple heads of dimension num_heads\n",
    "        batch_size, seq_len, d_model = x.size()\n",
    "        return x.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "    def combine_heads(self, x):   #combine the splitted heads to their original shape\n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "        \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        # applying linear transformation and splitting heads to the q,k,v\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "        \n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)  #apply the dot product\n",
    "        \n",
    "        output = self.W_o(self.combine_heads(attn_output)) #apply linear transformation and combine heads\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Encoder Block Structure:**  \n",
    "- Constructor takes four inputs: d_model, num_heads, d_ff, dropout\n",
    "- the input is passed to the self attention mechanism\n",
    "- adding residual connection to the self attention output and normalizing it\n",
    "- passing the output to the feed forward network\n",
    "- adding another residual connection\n",
    "- applying normalization to stabilize the final output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoder Block\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)  \n",
    "        self.feed_forward = FeedForwardBlock(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        attn_output = self.self_attn(x, x, x, mask)  #self attention mechanism\n",
    "        x = self.norm1(x + self.dropout(attn_output))  # residual connection and layer normalization\n",
    "        ff_output = self.feed_forward(x)   # feed forward network\n",
    "        x = self.norm2(x + self.dropout(ff_output))   #residual connection and layer normalization\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decoder Block Structure:**  \n",
    "- Constructor takes four inputs: d_model, num_heads, d_ff, dropout\n",
    "- the input is passed to the self attention mechanism\n",
    "- the output of the attention mechanism is passed to another instance of the mechanism 'cross_attn' to align the decoder output with relevant part of the input sequence\n",
    "- passing the cross attention output to the feed forwar network \n",
    "- applying normalization for stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decoder Block\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = FeedForwardBlock(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "        attn_output = self.self_attn(x, x, x, tgt_mask)  #self attention mechanism\n",
    "        x = self.norm1(x + self.dropout(attn_output))  #residual connection and normalization\n",
    "        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask) #cross attention with input as the encoder output\n",
    "        x = self.norm2(x + self.dropout(attn_output))  #residual connection and normalization\n",
    "        ff_output = self.feed_forward(x)  #feed forward network\n",
    "        x = self.norm3(x + self.dropout(ff_output))  #residual connection and normalization\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transformer Block Structure:**  \n",
    "- Constructor takes the following parameters: vocab_size: vocabulary size, num_classes : number of classes, d_model: model dimesionality, num_heads: number of attention heads, num_layers: number of encoder layers, d_ff: number of layers in the feed forward network, seq_len: maximum sequence length, dropout\n",
    "- the inputs: input_ids and attention_mask are passed to the model\n",
    "- they are passed to the word embedding layer and then to the positional encoding\n",
    "- the attention mask is applied to the output of the embeddings\n",
    "- the embedding are then passed to the encoder layers, with each layer applying multi-head self attention and feedforward\n",
    "- average pooling to calculate the average of each feature across seq_len\n",
    "- then passess to a fully connected layer to obtain the logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size, num_classes, d_model, num_heads, num_layers, d_ff, seq_len, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)  #word embedding\n",
    "        self.positional_encoding = PositionalEncoding(d_model, seq_len,dropout)  #positional encoding layer\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])  #encoder layers\n",
    "\n",
    "        self.fc = nn.Linear(d_model, num_classes)  #Fully connected layer\n",
    "        self.dropout = nn.Dropout(dropout)  # Dropout for regularization\n",
    "\n",
    "    def generate_mask(self, src):  #Generating mask to deal with paddings\n",
    "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
    "        return src_mask\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        src_mask = self.generate_mask(input_ids)  #generating the mask\n",
    "        src_embedded = self.dropout(self.positional_encoding(self.embedding(input_ids)))  #applying embbeding, positional encoding and dropout\n",
    "\n",
    "        # Apply the attention mask to the embeddings\n",
    "        src_embedded = src_embedded * attention_mask.unsqueeze(-1)  #applying attention mask to the input embeddings\n",
    "\n",
    "        enc_output = src_embedded\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output, src_mask)  #passing the input to the encoder layers\n",
    "\n",
    "        avg_pooled = torch.mean(enc_output, dim=1)  #average pooling\n",
    "\n",
    "        logits = self.fc(avg_pooled)  #passing the average pooled to the fully connected layer to output the logits\n",
    "        return logits\n",
    "    #logits unnormalized predicitions for each class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** I tried my best to research and fully understand each block of the transformer and how it works, but in implementing the code there may be some methods and some concepts that I don't fully understand its implementation or the mathematical intuition behind it. I researched and checked multiple resources and references to make sure that I implement it the correct way mentioned in these references to make it work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Preparing Train/Test Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this section I will take a 60% sample from the entire dataset to work with  \n",
    "- split the data into train and test data\n",
    "- pass the text to the bert tokenizer for text tokenization  \n",
    " - pass the tokenized text to the data loader to shuffle and load the data in batches for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping the id and tokenized text columns\n",
    "balanced_df = balanced_df.drop('id', axis=1)\n",
    "balanced_df = balanced_df.drop('tokenized_txt', axis=1)\n",
    "balanced_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### **Dataframe now contains the comment texts and the labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(balanced_df) #checking lenght of the data before taking a sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a 60% sample from the dataset for easy processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage = 60\n",
    "num_rows = int(len(balanced_df) * (percentage / 100))\n",
    "selected_rows = balanced_df.sample(n=num_rows, random_state=42)\n",
    "sample_df = pd.DataFrame(selected_rows)\n",
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sample_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data length now is 19470 instead of 32450"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "train_txt, test_txt, train_lbl, test_lbl = train_test_split(sample_df['comment_text'], sample_df.iloc[:,2:], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting the text data to a list of comments\n",
    "train_txt = train_txt.tolist()\n",
    "test_txt = test_txt.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting the labels to tensors\n",
    "train_labels = torch.tensor(train_lbl.values.tolist())\n",
    "test_labels = torch.tensor(test_lbl.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Bert tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 128  #Maximum sequence length\n",
    "\n",
    "# Tokenize the training set\n",
    "train_encodings = tokenizer(train_txt, max_length=seq_len, truncation=True, padding=True, return_tensors='pt')\n",
    "# Tokenize the testing set\n",
    "test_encodings = tokenizer(test_txt, max_length=seq_len , truncation=True, padding=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the DataLoader for the training set\n",
    "train_dataset = torch.utils.data.TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], train_labels)\n",
    "train_Data_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the DataLoader for the test set\n",
    "test_dataset = torch.utils.data.TensorDataset(test_encodings['input_ids'], test_encodings['attention_mask'], test_labels)\n",
    "test_Data_loader = DataLoader(test_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is now loaded in the data loader for shuffling and batching the data for the training and evaluation loops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Model Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer input arguments:  \n",
    "- vocab size: vocab size from the bert pre trained model\n",
    "- num_classes = number of classes in the data : 6\n",
    "- d_model: 128\n",
    "- num_heads: 8\n",
    "- num_layers = 8\n",
    "- d_ff = 2048\n",
    "- seq_len = 128\n",
    "- dropout: 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model and optimizer\n",
    "model = Transformer(vocab_size=len(tokenizer), num_classes=len(train_labels[0]), d_model=128, num_heads=8, num_layers=8, d_ff=2048, seq_len=seq_len, dropout=0.2)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "#criterion = nn.BCEWithLogitsLoss()  # Binary Cross-Entropy Loss for multi-label classification\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop Structure:  \n",
    "- defining the number of epochs\n",
    "- sending the model to the GPU if available\n",
    "- setting the model to training mode and iterating through epochs\n",
    "- iterate through batches of data using the data loader\n",
    "- Forward pass to make prediction\n",
    "- calculate loss\n",
    "- Backpropagation\n",
    "- Forward pass to update parameters\n",
    "- calculate the average loss per epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training Loop\n",
    "\n",
    "epochs = 10  #number of training epochs\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") #Check if a supported GPU is available\n",
    "model.to(device)  # move the model to the GPU\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()  #setting model in training mode\n",
    "    total_loss = 0  #defining the total loss for an epoch\n",
    "    for batch in train_Data_loader:  #iterate through data in batches using the data loader\n",
    "        \n",
    "        #inputs = [batch[0].to(device), batch[1].to(device)]\n",
    "        labels_float = batch[2].to(torch.float32)              #target labels converted to float as it gives error without the conversion\n",
    "\n",
    "        #inputs = {'input_ids': batch[0].to(device),\n",
    "                  #'attention_mask': batch[1].to(device),\n",
    "                  #'labels': batch[2].to(device)}\n",
    "        \n",
    "        input_ids = batch[0].to(device)      #input IDs\n",
    "        attention_mask = batch[1].to(device)   #attention mask\n",
    "        #labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()  #setting the gradients in the optimizer to zero\n",
    "        output = model(input_ids, attention_mask)  #Forward pass\n",
    "        loss = criterion(output, labels_float.to(device))  #Calculate the loss\n",
    "        loss.backward()  #Backpropagation\n",
    "        optimizer.step()  #Forward pass to update the paramters\n",
    "\n",
    "        total_loss += loss.item()  #sum the loss per batch\n",
    "    avg_loss = total_loss / len(train_Data_loader)   #average loss per epoch\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}\")  #printing epochs and average loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model now trained for 10 epochs on the training data and ready for evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Model Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation loop structure:  \n",
    "- Set the model to evaluation mode\n",
    "- initialize true and prediction lists\n",
    "- disabling the gradient computation for no backpropagation\n",
    "- Iterate through data batches using the data loader\n",
    "- Forward pass to make predictions\n",
    "- Extract logits which are the raw scores made by the model for each class\n",
    "- Calculate prediction\n",
    "- Compute the evaluation metrics for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "\n",
    "model.eval()  #Setting the model to evaluation mode\n",
    "y_true, y_pred = [], []  #lists to store true and predicted labels\n",
    "\n",
    "with torch.no_grad():  #disabling gradient computation as we don't need backpropagation\n",
    "\n",
    "    for batch in test_Data_loader:  #Iterate through batches of data using the dataloader\n",
    "\n",
    "        input_ids = batch[0].to(device)  #input IDs\n",
    "        attention_mask = batch[1].to(device) #Attention mask\n",
    "    \n",
    "        outputs = model(input_ids, attention_mask) #Forward pass to make the predictions\n",
    "        predictions = (torch.sigmoid(outputs) > 0.5).cpu().numpy()  #calculate predictions\n",
    "\n",
    "        #logits = outputs.detach().cpu().numpy() #Extract logits from model output\n",
    "        #predictions = np.argmax(logits, axis=1)   # make prediction by selecting class with highest logit score\n",
    "\n",
    "        y_true.extend(batch[2].cpu().numpy())  #true labels in a list\n",
    "        y_pred.extend(predictions)    # predictions in a list\n",
    "\n",
    "# Compute the metrics\n",
    "micro_precision = precision_score(y_true, y_pred, average='micro')\n",
    "micro_recall = recall_score(y_true, y_pred, average='micro')\n",
    "micro_f1 = f1_score(y_true, y_pred, average='micro')\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print('micro_precision' , micro_precision)\n",
    "print('micro_recall' , micro_recall)\n",
    "print('micro_f1' , micro_f1)\n",
    "print('accuracy' , accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_true)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Final Notes**: The model at first try got very low scores (0.02). I tried then revising data cleaning and resolved the data imbalance problem, I also tried to change a bit with the model input arguments and the accuracy is now (0.12) still very low but it doesn't change much even when changing arguments. I tried alot but can't find a solution for the problem currently. I suspect the problem may be either in the evaluation loop or the Transformer() class.  \n",
    "\n",
    "Finally, I understand the full code and how a transformer works, but there may be some methods and modules that I don't 100% understand how they work and why to choose them specifically and also the mathematical intuition behind some operations. Thank you"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
